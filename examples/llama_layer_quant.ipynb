{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../smoothquant')\n",
    "from smooth import *\n",
    "from fake_quant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaAttention,\n",
    "    LlamaDecoderLayer,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaMLP,\n",
    ")\n",
    "from transformers import LlamaTokenizer\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.fake_quant import quantize_llama_like\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.dataset = tokenizer(\n",
    "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        nlls = []\n",
    "        for i in tqdm.tqdm(range(self.n_samples), desc=\"Evaluating...\"):\n",
    "            batch = self.dataset[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.dataset[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (self.n_samples * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master_112/m56121041/miniconda3/envs/smoothquant/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "evaluator = Evaluator(dataset, tokenizer, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "固定樣本文字: \n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def get_act_scales(model, sample_text):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    act_scales_all = {}  # Dictionary to store the activations of all layers\n",
    "\n",
    "    def stat_tensor(name, tensor):\n",
    "        hidden_dim = tensor.shape[-1]\n",
    "        # shape: (batch_size, seq_len, hidden_dim) -> (batch_size * seq_len, hidden_dim)\n",
    "        # clone() is used to prevent modifying the original tensor\n",
    "        tensor_original = tensor.clone().view(-1, hidden_dim).detach()\n",
    "        act_scales_all[name] = tensor_original\n",
    "\n",
    "    # module: is the layer being hooked\n",
    "    def stat_input_hook(module, input, output, name):\n",
    "        input_tensor = input[0] if isinstance(input, tuple) else input\n",
    "        stat_tensor(name, input_tensor)\n",
    "\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        # if isinstance(module, (nn.Linear, W8A8Linear)):\n",
    "        if isinstance(module, (nn.Linear, W8A8Linear, W4A4Linear)):\n",
    "            print(f\"Hooking layer: {name}\")\n",
    "            hook = module.register_forward_hook(\n",
    "                functools.partial(stat_input_hook, name=name)\n",
    "            )\n",
    "            hooks.append(hook)\n",
    "\n",
    "    # Tokenize the sample\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    input_ids = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=1024).input_ids\n",
    "\n",
    "    # Move input_ids to the same device as the model\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        model(input_ids)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return act_scales_all\n",
    "\n",
    "# 固定樣本文字\n",
    "def get_sample_text():\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    sample = dataset[torch.randint(len(dataset), (1,)).item()][\"text\"]\n",
    "    print(\"固定樣本文字:\", sample)  # 查看固定的樣本文字\n",
    "    return sample\n",
    "\n",
    "# Example usage\n",
    "sample_text = get_sample_text()  # 第一次執行時抽取樣本\n",
    "# act_scales = get_act_scales(model, sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantizing 0~3layer to 4bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fd953f3085453d94a6be7edc4ccc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_fp16 = LlamaForCausalLM.from_pretrained(\n",
    "    \"../../llama-2-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer 0\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 0\n",
      "Processing layer 1\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 1\n",
      "Processing layer 2\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 2\n",
      "Processing layer 3\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Processing layer 3\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 5\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 6\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 7\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 8\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 9\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 10\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 11\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 12\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 13\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 14\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 15\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 16\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 17\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 18\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 19\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 20\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 21\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 22\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 23\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 24\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 25\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 26\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 27\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 28\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 29\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n"
     ]
    }
   ],
   "source": [
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm_layer(model_fp16, act_scales, 0.85,  quant_layers=range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 0\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 1\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 2\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Quantizing layer 3\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 5 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 6 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 7 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 8 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 9 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 10 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 11 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 12 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 13 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 14 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 15 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 16 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 17 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 18 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 19 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 20 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 21 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 22 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 23 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 24 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 25 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 26 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 27 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 28 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 29 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n"
     ]
    }
   ],
   "source": [
    "model_smoothquant_layer0_3 = quantize_llama_like_layer(model_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (1): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (2): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (3): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (27): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (29): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (30): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (31): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_smoothquant_layer0_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 40/40 [00:20<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model (fp16) perplexity: 168.82559204101562\n"
     ]
    }
   ],
   "source": [
    "smoothquant_layer0_3 = evaluator.evaluate(model_smoothquant_layer0_3)\n",
    "print(f\"Original model (fp16) perplexity: {smoothquant_layer0_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize layer 5~29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd6c7db68a1423d8ac008f7e8fe2900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_fp16 = LlamaForCausalLM.from_pretrained(\n",
    "    \"../../llama-2-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 0\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 1\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 2\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 3\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Skipping layer 4\n",
      "Processing layer 5\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 5\n",
      "Processing layer 6\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 6\n",
      "Processing layer 7\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 7\n",
      "Processing layer 8\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 8\n",
      "Processing layer 9\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 9\n",
      "Processing layer 10\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 10\n",
      "Processing layer 11\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 11\n",
      "Processing layer 12\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 12\n",
      "Processing layer 13\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 13\n",
      "Processing layer 14\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 14\n",
      "Processing layer 15\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 15\n",
      "Processing layer 16\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 16\n",
      "Processing layer 17\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 17\n",
      "Processing layer 18\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 18\n",
      "Processing layer 19\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 19\n",
      "Processing layer 20\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 20\n",
      "Processing layer 21\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 21\n",
      "Processing layer 22\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 22\n",
      "Processing layer 23\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 23\n",
      "Processing layer 24\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 24\n",
      "Processing layer 25\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 25\n",
      "Processing layer 26\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 26\n",
      "Processing layer 27\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 27\n",
      "Processing layer 28\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 28\n",
      "Processing layer 29\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=4096, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Module: Linear(in_features=4096, out_features=11008, bias=False), Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Processing layer 29\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 30\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n",
      "Skipping layer 31\n"
     ]
    }
   ],
   "source": [
    "act_scales = torch.load(\"../act_scales/llama-2-7b.pt\")\n",
    "smooth_lm_layer(model_fp16, act_scales, 0.85,  quant_layers=range(5,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 0 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 1 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 2 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 3 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Skipping layer 4 for quantization\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 5\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 6\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 7\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 8\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 9\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 10\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 11\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 12\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 13\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 14\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 15\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 16\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 17\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 18\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 19\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 20\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 21\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 22\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 23\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 24\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 25\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 26\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 27\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 28\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Quantizing layer 29\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 30 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n",
      "Skipping layer 31 for quantization\n"
     ]
    }
   ],
   "source": [
    "model_smoothquant_layer4_29 = quantize_llama_like_layer(model_fp16, quant_layers=range(5,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (1): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (2): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (3): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (27): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (29): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (k_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (v_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (o_proj): W4A4Linear(4096, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (up_proj): W4A4Linear(4096, 11008, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (down_proj): W4A4Linear(11008, 4096, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (30): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (31): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_smoothquant_layer4_29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model (fp16) perplexity: 1427.8328857421875\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
      "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
      "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
     ]
    }
   ],
   "source": [
    "ppl_smoothquant_layer4_29 = evaluator.evaluate(model_smoothquant_layer4_29)\n",
    "print(f\"Original model (fp16) perplexity: {ppl_smoothquant_layer4_29}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoothquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
